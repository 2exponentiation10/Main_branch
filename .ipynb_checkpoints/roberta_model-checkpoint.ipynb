{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from glob import glob\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm, tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"Data/processed_train2.csv\")\n",
    "test= pd.read_csv(\"Data/processed_test2.csv\")\n",
    "submission = pd.read_csv(\"Data/submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>conversation</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>지금 당장 뉴스 기사 내가 불러준 대로 보도 해  사실 확인이 되지 않은 기사는 낼...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>이 버러지 같은 게 너 내가 누군 줄 알아  손님 욕하시면 안 됩니다   어디서 말...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>공책 돌려받길 원하면 빨리 뛰어봐  이 굼벵아  빨리 내놔  빨릐 내놓아야  빨리 ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>사장님 저기 말할게 있는데요 뭔데 임마 아니 우리 게임 회사는 전체이용가 게임이잖아...</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>죽고 싶어서 환장했어  왜 이렇게 말을 안 들어   죄송해요  한 번만 봐주세요  ...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>야 이 시계 예쁜데 생일선물로 받은거야 이거 나 주고 가라 안돼 이게 어디 까불어 ...</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>새로 생긴 카페 가봤어  아니  아직 안 가봤어  거기 커피 진짜 맛있어  그래  ...</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>야 너 이리 와봐 이게 뭐야  네  뭐가 문제가 생겼나요 이새끼봐라 니가 뭘 했다고...</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>아니 지금 장난해  보고서가 이따위가 뭐야 지금  죄송합니다   죄송이고 나발이고 ...</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>형 그러지 마  왜 갑자기 나한테 그래 너야 말로 미쳤어  너 때문에 우리 다 망하...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        conversation  class\n",
       "0  지금 당장 뉴스 기사 내가 불러준 대로 보도 해  사실 확인이 되지 않은 기사는 낼...    0.0\n",
       "1  이 버러지 같은 게 너 내가 누군 줄 알아  손님 욕하시면 안 됩니다   어디서 말...    1.0\n",
       "2  공책 돌려받길 원하면 빨리 뛰어봐  이 굼벵아  빨리 내놔  빨릐 내놓아야  빨리 ...    1.0\n",
       "3  사장님 저기 말할게 있는데요 뭔데 임마 아니 우리 게임 회사는 전체이용가 게임이잖아...    3.0\n",
       "4  죽고 싶어서 환장했어  왜 이렇게 말을 안 들어   죄송해요  한 번만 봐주세요  ...    0.0\n",
       "5  야 이 시계 예쁜데 생일선물로 받은거야 이거 나 주고 가라 안돼 이게 어디 까불어 ...    2.0\n",
       "6  새로 생긴 카페 가봤어  아니  아직 안 가봤어  거기 커피 진짜 맛있어  그래  ...    4.0\n",
       "7  야 너 이리 와봐 이게 뭐야  네  뭐가 문제가 생겼나요 이새끼봐라 니가 뭘 했다고...    3.0\n",
       "8  아니 지금 장난해  보고서가 이따위가 뭐야 지금  죄송합니다   죄송이고 나발이고 ...    3.0\n",
       "9  형 그러지 마  왜 갑자기 나한테 그래 너야 말로 미쳤어  너 때문에 우리 다 망하...    0.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4552, 2)\n",
      "(500, 1)\n",
      "(500, 2)\n"
     ]
    }
   ],
   "source": [
    "print(train.shape) #train 약 45백개\n",
    "print(test.shape) # test 약 5백개\n",
    "print(submission.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0    1011\n",
      "2.0     973\n",
      "3.0     970\n",
      "0.0     892\n",
      "4.0     706\n",
      "Name: class, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "# 데이터 분포 꽤나 balance !! \n",
    "feature = train['class']\n",
    "\n",
    "plt.figure(figsize=(7,5)) \n",
    "plt.title('class', fontsize=20)\n",
    "temp = feature.value_counts() # feature 변수의 변수별 개수 계산\n",
    "plt.bar(temp.keys(), temp.values, width=0.5)\n",
    "plt.xticks(temp.keys(), fontsize=15) \n",
    "plt.show()\n",
    "print(temp) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Conversation Length:  885\n",
      "Min Conversation Length:  41\n",
      "Mean Conversation Lenght:  257.6258787346221\n"
     ]
    }
   ],
   "source": [
    "print('Max Conversation Length: ', np.max(train['conversation'].str.len()))  #전제 최대는 90자, 최소는 19자\n",
    "print('Min Conversation Length: ', np.min(train['conversation'].str.len()))\n",
    "print('Mean Conversation Lenght: ',np.mean(train['conversation'].str.len()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from transformers import AutoModelForSequenceClassification, AutoConfig, AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SEED 고정, GPU설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "def seed_everything(seed:int = 1004):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)  # type: ignore\n",
    "    torch.backends.cudnn.deterministic = True  # type: ignore\n",
    "    torch.backends.cudnn.benchmark = True  # type: ignore\n",
    "\n",
    "seed_everything(42)\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "klue/roberta-large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9234c15e49041d6842ce77edcffbd9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/547 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5eef2122556940fd897d2c753ce96dd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.25G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at klue/roberta-large were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.decoder.bias', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at klue/roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3b510cc3526454eb009c89b6cf0cc1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/375 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18a21bb506044b149b25482fd1d1d946",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/243k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94866b79e32541edab6e3a98e9945126",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/734k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb7eee35fd89401fb6bdcaf7cb7f1382",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/173 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "model = AutoModel.from_pretrained(\"klue/roberta-large\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"klue/roberta-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SPLIT \n",
    "train, val = train_test_split(train, test_size=0.2, random_state=2021)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>conversation</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>714</th>\n",
       "      <td>야 너 이 새끼 뭐 사러 가냐  네  왜요  너 오늘 돈 많아 보인다 좀 살면 돈 ...</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>388</th>\n",
       "      <td>도저히 상황이 안돼서 말씀하신 기간까지 마련하지 못한 점 죄송하게 생각합네다  그래...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3549</th>\n",
       "      <td>빨리빨리 일 처리 안 해   죄송합니다  김대리 이번에는 승진해야지 죄송합니다 빨리...</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1874</th>\n",
       "      <td>요새 애들은 소갈머리가 없어 맞아요 부장님 죄송합니다 나때는 고개도 못들었어 난 눈...</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1667</th>\n",
       "      <td>이번 주말에 여행 갈까  어디로  해변으로 가고 싶어  좋아  그럼 계획 세워보자 ...</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           conversation  class\n",
       "714   야 너 이 새끼 뭐 사러 가냐  네  왜요  너 오늘 돈 많아 보인다 좀 살면 돈 ...    2.0\n",
       "388   도저히 상황이 안돼서 말씀하신 기간까지 마련하지 못한 점 죄송하게 생각합네다  그래...    0.0\n",
       "3549  빨리빨리 일 처리 안 해   죄송합니다  김대리 이번에는 승진해야지 죄송합니다 빨리...    3.0\n",
       "1874  요새 애들은 소갈머리가 없어 맞아요 부장님 죄송합니다 나때는 고개도 못들었어 난 눈...    3.0\n",
       "1667  이번 주말에 여행 갈까  어디로  해변으로 가고 싶어  좋아  그럼 계획 세워보자 ...    4.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>아가씨 담배 한 갑 주소 네 4 500원입니다  어 네 지갑 어디 갔지  에이 버스...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>우리 팀에서 다른 팀으로 갈 사람 없나  그럼  영지씨가 가는 건 어때  네  제가...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>너 오늘 그게 뭐야 네 제가 뭘 잘못했나요  제대로 좀 하지 네 똑바로 좀 하지 행...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>이거 들어봐 와 이 노래 진짜 좋다 그치 요즘 이것만 들어 진짜 너무 좋다 내가 요...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>아무튼 앞으로 네가 내 와이파이야  응 와이파이 온  켰어  반말  주인님이라고도 ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  아가씨 담배 한 갑 주소 네 4 500원입니다  어 네 지갑 어디 갔지  에이 버스...\n",
       "1  우리 팀에서 다른 팀으로 갈 사람 없나  그럼  영지씨가 가는 건 어때  네  제가...\n",
       "2  너 오늘 그게 뭐야 네 제가 뭘 잘못했나요  제대로 좀 하지 네 똑바로 좀 하지 행...\n",
       "3  이거 들어봐 와 이 노래 진짜 좋다 그치 요즘 이것만 들어 진짜 너무 좋다 내가 요...\n",
       "4  아무튼 앞으로 네가 내 와이파이야  응 와이파이 온  켰어  반말  주인님이라고도 ..."
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "from sklearn import preprocessing\n",
    "import tensorflow as tf\n",
    "from transformers import BertTokenizer, TFBertModel\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at klue/roberta-large were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.decoder.bias', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at klue/roberta-large and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "config = AutoConfig.from_pretrained('klue/roberta-large')\n",
    "config.num_labels = 3\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained('klue/roberta-large', config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_len = 128  # 해당 길이를 초과하는 단어에 대해선 bert가 학습하지 않음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_examples_to_features(sent_list, max_seq_len, tokenizer):\n",
    "\n",
    "    input_ids, attention_masks, token_type_ids = [], [], []\n",
    "\n",
    "    for sent in tqdm(sent_list, total=len(sent_list)):\n",
    "        encoding_result = tokenizer.encode_plus(\n",
    "            sent,\n",
    "            max_length=max_seq_len,\n",
    "            pad_to_max_length=True,\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_token_type_ids=True\n",
    "        )\n",
    "\n",
    "        input_ids.append(encoding_result['input_ids'])\n",
    "        attention_masks.append(encoding_result['attention_mask'])\n",
    "        token_type_ids.append(encoding_result['token_type_ids'])\n",
    "\n",
    "    input_ids = np.array(input_ids, dtype=int)\n",
    "    attention_masks = np.array(attention_masks, dtype=int)\n",
    "    token_type_ids = np.array(token_type_ids, dtype=int)\n",
    "\n",
    "    return (input_ids, attention_masks, token_type_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3641 [00:00<?, ?it/s]/opt/conda/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "100%|██████████| 3641/3641 [00:01<00:00, 2342.44it/s]\n"
     ]
    }
   ],
   "source": [
    "X_train = convert_examples_to_features(train['conversation'],  max_seq_len=max_seq_len, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어에 대한 정수 인코딩 : [    0   732  2116  4022   733   568  1376  2203  2275   732   780  6233\n",
      "   752  2116  5390  2470   568  1170  2180  2778  1376  1170  6074  3866\n",
      "  8835  3637  2728  2073  2227  2203  2275  1110  2069  2113  2116  2899\n",
      "  2062  6148  2097  3690  2259  1378  5014  2318   743  1504  7171 24402\n",
      "  1169   732  2116  3842  8705  3949 12421  2170  3839  2015  2529  3949\n",
      " 23548   770  2052   717  2079  6045  2470  1389  2057  2138  3914  4577\n",
      "  2529  6791  2154  7624  2097  2223  2181  4458  6892  2170  5331  4962\n",
      "  2371  2088   752  4369  1875  2151  2042  2066  3679  2201  2203  2965\n",
      "  3869  2097     2     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1]\n",
      "어텐션 마스크 : [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "세그먼트 인코딩 : [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "각 인코딩의 길이 : 128\n",
      "정수 인코딩 복원 : [CLS] 내가 사고 낸 거 아네야 내 눈으로 네가 운전한 거 봤거든 아 봤구나 그런데 일부러 그런것은아네야 믿을수가없다 미안해 다시는 안 그럴게 너 이 새끼 두고 봐 내가 어떻게하나 친구사이에 이러기냐 친구라는 놈이 나의 소중한 애마를 그렇게 만드냐 한번만 용서해주라 일단 경찰서에 증거 제출했고 네 가정 풍비박산 만들테네깐 기대해 [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n"
     ]
    }
   ],
   "source": [
    "# 최대 길이: 128\n",
    "input_id = X_train[0][0]\n",
    "attention_mask = X_train[1][0]\n",
    "token_type_id = X_train[2][0]\n",
    "\n",
    "print('단어에 대한 정수 인코딩 :',input_id)\n",
    "print('어텐션 마스크 :',attention_mask)\n",
    "print('세그먼트 인코딩 :',token_type_id)\n",
    "print('각 인코딩의 길이 :', len(input_id))\n",
    "print('정수 인코딩 복원 :',tokenizer.decode(input_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 911/911 [00:00<00:00, 2312.91it/s]\n"
     ]
    }
   ],
   "source": [
    "X_valid = convert_examples_to_features(val['conversation'], max_seq_len=max_seq_len, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:00<00:00, 2376.65it/s]\n"
     ]
    }
   ],
   "source": [
    "X_test = convert_examples_to_features(test['text'],max_seq_len=max_seq_len, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label = train['class'].tolist()\n",
    "val_label = val['class'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0.0: 0, 1.0: 1, 2.0: 2, 3.0: 3, 4.0: 4}\n",
      "{0: 0.0, 1: 1.0, 2: 2.0, 3: 3.0, 4: 4.0}\n"
     ]
    }
   ],
   "source": [
    "idx_encode = preprocessing.LabelEncoder()\n",
    "idx_encode.fit(train_label)\n",
    "\n",
    "y_train = idx_encode.transform(train_label) # 주어진 고유한 정수로 변환\n",
    "y_val = idx_encode.transform(val_label) # 고유한 정수로 변환\n",
    "\n",
    "label_idx = dict(zip(list(idx_encode.classes_), idx_encode.transform(list(idx_encode.classes_))))\n",
    "idx_label = {value: key for key, value in label_idx.items()}\n",
    "print(label_idx)\n",
    "print(idx_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TFBertForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"'klue/roberta-large'\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.initializers import TruncatedNormal\n",
    "from transformers import TFAutoModel\n",
    "\n",
    "class Klue_RobertaClassifier(Model):\n",
    "    def __init__(self, model_name):\n",
    "        super(Klue_RobertaClassifier, self).__init__()\n",
    "        self.bert = TFAutoModel.from_pretrained(\"klue/roberta-large\", num_labels=5, from_pt=True)\n",
    "        self.dropout = tf.keras.layers.Dropout(rate=0.5)\n",
    "        self.classifier = Dense(3, kernel_initializer=TruncatedNormal(0.02),activation='softmax') #가중치 초기화, 활성화함수\n",
    "\n",
    "    def call(self, inputs, attention_mask=None, token_type_ids=None, training=False):\n",
    "        input_ids, attention_mask, token_type_ids=inputs\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "        pooled_output = outputs[1]\n",
    "        pooled_output = self.dropout(pooled_output, training=training)\n",
    "        prediction = self.classifier(pooled_output)\n",
    "\n",
    "        return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFRobertaModel: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.decoder.bias', 'roberta.embeddings.position_ids', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing TFRobertaModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFRobertaModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights or buffers of the TF 2.0 model TFRobertaModel were not initialized from the PyTorch model and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = Klue_RobertaClassifier(\"klue/roberta-large\")  #dropout=0.5까지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamWeightDecay\n",
    "optimizer = AdamWeightDecay(1e-5, weight_decay_rate=1e-4)  #과적합 방지\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=[metric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/keras/backend.py:4906: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": " failed to allocate memory\n\t [[node klue__roberta_classifier_1/tf_roberta_model_1/roberta/encoder/layer_._22/attention/output/LayerNorm/batchnorm/mul_2 (defined at opt/conda/lib/python3.9/site-packages/transformers/models/roberta/modeling_tf_roberta.py:281) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n [Op:__inference_train_function_115043]\n\nFunction call stack:\ntrain_function\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_245/1473305745.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m     patience=2)\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m model.fit(\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     callbacks = [early_stopping])\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1182\u001b[0m                 _r=1):\n\u001b[1;32m   1183\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1184\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1185\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1186\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    883\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 885\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    886\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    948\u001b[0m         \u001b[0;31m# Lifting succeeded, so variables are initialized and we can run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m         \u001b[0;31m# stateless function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 950\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    951\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m       \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiltered_flat_args\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3037\u001b[0m       (graph_function,\n\u001b[1;32m   3038\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m-> 3039\u001b[0;31m     return graph_function._call_flat(\n\u001b[0m\u001b[1;32m   3040\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m   3041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1961\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1962\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1963\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1964\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1965\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    589\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 591\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    592\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    593\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m:  failed to allocate memory\n\t [[node klue__roberta_classifier_1/tf_roberta_model_1/roberta/encoder/layer_._22/attention/output/LayerNorm/batchnorm/mul_2 (defined at opt/conda/lib/python3.9/site-packages/transformers/models/roberta/modeling_tf_roberta.py:281) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n [Op:__inference_train_function_115043]\n\nFunction call stack:\ntrain_function\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor=\"val_accuracy\", \n",
    "    min_delta=0.001,\n",
    "    patience=2)\n",
    "\n",
    "model.fit(\n",
    "    X_train, y_train, epochs=3, batch_size=8, validation_data=(X_valid, y_val),\n",
    "    callbacks = [early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = [np.argmax(val) for val in pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = [list(label_idx.keys())[_] for _ in result]\n",
    "out[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['class'] = out\n",
    "test[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['class'].value_counts() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission['class']=out\n",
    "submission.to_csv(\"klue-roberta-large-model.csv\", index=False)  #0.835"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
