{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from glob import glob\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm, tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"Data/processed_train2.csv\")\n",
    "test= pd.read_csv(\"Data/processed_test2.csv\")\n",
    "submission = pd.read_csv(\"Data/submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>conversation</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>지금 당장 뉴스 기사 내가 불러준 대로 보도 해  사실 확인이 되지 않은 기사는 낼...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>이 버러지 같은 게 너 내가 누군 줄 알아  손님 욕하시면 안 됩니다   어디서 말...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>공책 돌려받길 원하면 빨리 뛰어봐  이 굼벵아  빨리 내놔  빨릐 내놓아야  빨리 ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>사장님 저기 말할게 있는데요 뭔데 임마 아니 우리 게임 회사는 전체이용가 게임이잖아...</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>죽고 싶어서 환장했어  왜 이렇게 말을 안 들어   죄송해요  한 번만 봐주세요  ...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>야 이 시계 예쁜데 생일선물로 받은거야 이거 나 주고 가라 안돼 이게 어디 까불어 ...</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>새로 생긴 카페 가봤어  아니  아직 안 가봤어  거기 커피 진짜 맛있어  그래  ...</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>야 너 이리 와봐 이게 뭐야  네  뭐가 문제가 생겼나요 이새끼봐라 니가 뭘 했다고...</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>아니 지금 장난해  보고서가 이따위가 뭐야 지금  죄송합니다   죄송이고 나발이고 ...</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>형 그러지 마  왜 갑자기 나한테 그래 너야 말로 미쳤어  너 때문에 우리 다 망하...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        conversation  class\n",
       "0  지금 당장 뉴스 기사 내가 불러준 대로 보도 해  사실 확인이 되지 않은 기사는 낼...    0.0\n",
       "1  이 버러지 같은 게 너 내가 누군 줄 알아  손님 욕하시면 안 됩니다   어디서 말...    1.0\n",
       "2  공책 돌려받길 원하면 빨리 뛰어봐  이 굼벵아  빨리 내놔  빨릐 내놓아야  빨리 ...    1.0\n",
       "3  사장님 저기 말할게 있는데요 뭔데 임마 아니 우리 게임 회사는 전체이용가 게임이잖아...    3.0\n",
       "4  죽고 싶어서 환장했어  왜 이렇게 말을 안 들어   죄송해요  한 번만 봐주세요  ...    0.0\n",
       "5  야 이 시계 예쁜데 생일선물로 받은거야 이거 나 주고 가라 안돼 이게 어디 까불어 ...    2.0\n",
       "6  새로 생긴 카페 가봤어  아니  아직 안 가봤어  거기 커피 진짜 맛있어  그래  ...    4.0\n",
       "7  야 너 이리 와봐 이게 뭐야  네  뭐가 문제가 생겼나요 이새끼봐라 니가 뭘 했다고...    3.0\n",
       "8  아니 지금 장난해  보고서가 이따위가 뭐야 지금  죄송합니다   죄송이고 나발이고 ...    3.0\n",
       "9  형 그러지 마  왜 갑자기 나한테 그래 너야 말로 미쳤어  너 때문에 우리 다 망하...    0.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4552, 2)\n",
      "(500, 1)\n",
      "(500, 2)\n"
     ]
    }
   ],
   "source": [
    "print(train.shape) #train 약 45백개\n",
    "print(test.shape) # test 약 5백개\n",
    "print(submission.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 504x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0    1011\n",
      "2.0     973\n",
      "3.0     970\n",
      "0.0     892\n",
      "4.0     706\n",
      "Name: class, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "# 데이터 분포 꽤나 balance !! \n",
    "feature = train['class']\n",
    "\n",
    "plt.figure(figsize=(7,5)) \n",
    "plt.title('class', fontsize=20)\n",
    "temp = feature.value_counts() # feature 변수의 변수별 개수 계산\n",
    "plt.bar(temp.keys(), temp.values, width=0.5)\n",
    "plt.xticks(temp.keys(), fontsize=15) \n",
    "plt.show()\n",
    "print(temp) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Conversation Length:  885\n",
      "Min Conversation Length:  41\n",
      "Mean Conversation Lenght:  257.6258787346221\n"
     ]
    }
   ],
   "source": [
    "print('Max Conversation Length: ', np.max(train['conversation'].str.len()))  #전제 최대는 90자, 최소는 19자\n",
    "print('Min Conversation Length: ', np.min(train['conversation'].str.len()))\n",
    "print('Mean Conversation Lenght: ',np.mean(train['conversation'].str.len()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from transformers import AutoModelForSequenceClassification, AutoConfig, AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SEED 고정, GPU설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "def seed_everything(seed:int = 1004):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)  # type: ignore\n",
    "    torch.backends.cudnn.deterministic = True  # type: ignore\n",
    "    torch.backends.cudnn.benchmark = True  # type: ignore\n",
    "\n",
    "seed_everything(42)\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "klue/roberta-large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at klue/roberta-large were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.decoder.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at klue/roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "model = AutoModel.from_pretrained(\"klue/roberta-large\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"klue/roberta-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SPLIT \n",
    "train, val = train_test_split(train, test_size=0.2, random_state=2021)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>conversation</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>714</th>\n",
       "      <td>야 너 이 새끼 뭐 사러 가냐  네  왜요  너 오늘 돈 많아 보인다 좀 살면 돈 ...</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>388</th>\n",
       "      <td>도저히 상황이 안돼서 말씀하신 기간까지 마련하지 못한 점 죄송하게 생각합네다  그래...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3549</th>\n",
       "      <td>빨리빨리 일 처리 안 해   죄송합니다  김대리 이번에는 승진해야지 죄송합니다 빨리...</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1874</th>\n",
       "      <td>요새 애들은 소갈머리가 없어 맞아요 부장님 죄송합니다 나때는 고개도 못들었어 난 눈...</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1667</th>\n",
       "      <td>이번 주말에 여행 갈까  어디로  해변으로 가고 싶어  좋아  그럼 계획 세워보자 ...</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           conversation  class\n",
       "714   야 너 이 새끼 뭐 사러 가냐  네  왜요  너 오늘 돈 많아 보인다 좀 살면 돈 ...    2.0\n",
       "388   도저히 상황이 안돼서 말씀하신 기간까지 마련하지 못한 점 죄송하게 생각합네다  그래...    0.0\n",
       "3549  빨리빨리 일 처리 안 해   죄송합니다  김대리 이번에는 승진해야지 죄송합니다 빨리...    3.0\n",
       "1874  요새 애들은 소갈머리가 없어 맞아요 부장님 죄송합니다 나때는 고개도 못들었어 난 눈...    3.0\n",
       "1667  이번 주말에 여행 갈까  어디로  해변으로 가고 싶어  좋아  그럼 계획 세워보자 ...    4.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>아가씨 담배 한 갑 주소 네 4 500원입니다  어 네 지갑 어디 갔지  에이 버스...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>우리 팀에서 다른 팀으로 갈 사람 없나  그럼  영지씨가 가는 건 어때  네  제가...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>너 오늘 그게 뭐야 네 제가 뭘 잘못했나요  제대로 좀 하지 네 똑바로 좀 하지 행...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>이거 들어봐 와 이 노래 진짜 좋다 그치 요즘 이것만 들어 진짜 너무 좋다 내가 요...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>아무튼 앞으로 네가 내 와이파이야  응 와이파이 온  켰어  반말  주인님이라고도 ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  아가씨 담배 한 갑 주소 네 4 500원입니다  어 네 지갑 어디 갔지  에이 버스...\n",
       "1  우리 팀에서 다른 팀으로 갈 사람 없나  그럼  영지씨가 가는 건 어때  네  제가...\n",
       "2  너 오늘 그게 뭐야 네 제가 뭘 잘못했나요  제대로 좀 하지 네 똑바로 좀 하지 행...\n",
       "3  이거 들어봐 와 이 노래 진짜 좋다 그치 요즘 이것만 들어 진짜 너무 좋다 내가 요...\n",
       "4  아무튼 앞으로 네가 내 와이파이야  응 와이파이 온  켰어  반말  주인님이라고도 ..."
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "from sklearn import preprocessing\n",
    "import tensorflow as tf\n",
    "from transformers import BertTokenizer, TFBertModel\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at klue/roberta-large were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.decoder.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at klue/roberta-large and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "config = AutoConfig.from_pretrained('klue/roberta-large')\n",
    "config.num_labels = 3\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained('klue/roberta-large', config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_len = 128  # 해당 길이를 초과하는 단어에 대해선 bert가 학습하지 않음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_examples_to_features(sent_list, max_seq_len, tokenizer):\n",
    "\n",
    "    input_ids, attention_masks, token_type_ids = [], [], []\n",
    "\n",
    "    for sent in tqdm(sent_list, total=len(sent_list)):\n",
    "        encoding_result = tokenizer.encode_plus(\n",
    "            sent,\n",
    "            max_length=max_seq_len,\n",
    "            pad_to_max_length=True,\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_token_type_ids=True\n",
    "        )\n",
    "\n",
    "        input_ids.append(encoding_result['input_ids'])\n",
    "        attention_masks.append(encoding_result['attention_mask'])\n",
    "        token_type_ids.append(encoding_result['token_type_ids'])\n",
    "\n",
    "    input_ids = np.array(input_ids, dtype=int)\n",
    "    attention_masks = np.array(attention_masks, dtype=int)\n",
    "    token_type_ids = np.array(token_type_ids, dtype=int)\n",
    "\n",
    "    return (input_ids, attention_masks, token_type_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3641 [00:00<?, ?it/s]/opt/conda/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "100%|██████████| 3641/3641 [00:01<00:00, 2503.90it/s]\n"
     ]
    }
   ],
   "source": [
    "X_train = convert_examples_to_features(train['conversation'],  max_seq_len=max_seq_len, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어에 대한 정수 인코딩 : [    0   732  2116  4022   733   568  1376  2203  2275   732   780  6233\n",
      "   752  2116  5390  2470   568  1170  2180  2778  1376  1170  6074  3866\n",
      "  8835  3637  2728  2073  2227  2203  2275  1110  2069  2113  2116  2899\n",
      "  2062  6148  2097  3690  2259  1378  5014  2318   743  1504  7171 24402\n",
      "  1169   732  2116  3842  8705  3949 12421  2170  3839  2015  2529  3949\n",
      " 23548   770  2052   717  2079  6045  2470  1389  2057  2138  3914  4577\n",
      "  2529  6791  2154  7624  2097  2223  2181  4458  6892  2170  5331  4962\n",
      "  2371  2088   752  4369  1875  2151  2042  2066  3679  2201  2203  2965\n",
      "  3869  2097     2     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1]\n",
      "어텐션 마스크 : [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "세그먼트 인코딩 : [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "각 인코딩의 길이 : 128\n",
      "정수 인코딩 복원 : [CLS] 내가 사고 낸 거 아네야 내 눈으로 네가 운전한 거 봤거든 아 봤구나 그런데 일부러 그런것은아네야 믿을수가없다 미안해 다시는 안 그럴게 너 이 새끼 두고 봐 내가 어떻게하나 친구사이에 이러기냐 친구라는 놈이 나의 소중한 애마를 그렇게 만드냐 한번만 용서해주라 일단 경찰서에 증거 제출했고 네 가정 풍비박산 만들테네깐 기대해 [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n"
     ]
    }
   ],
   "source": [
    "# 최대 길이: 128\n",
    "input_id = X_train[0][0]\n",
    "attention_mask = X_train[1][0]\n",
    "token_type_id = X_train[2][0]\n",
    "\n",
    "print('단어에 대한 정수 인코딩 :',input_id)\n",
    "print('어텐션 마스크 :',attention_mask)\n",
    "print('세그먼트 인코딩 :',token_type_id)\n",
    "print('각 인코딩의 길이 :', len(input_id))\n",
    "print('정수 인코딩 복원 :',tokenizer.decode(input_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 911/911 [00:00<00:00, 2503.21it/s]\n"
     ]
    }
   ],
   "source": [
    "X_valid = convert_examples_to_features(val['conversation'], max_seq_len=max_seq_len, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:00<00:00, 2807.99it/s]\n"
     ]
    }
   ],
   "source": [
    "X_test = convert_examples_to_features(test['text'],max_seq_len=max_seq_len, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label = train['class'].tolist()\n",
    "val_label = val['class'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0.0: 0, 1.0: 1, 2.0: 2, 3.0: 3, 4.0: 4}\n",
      "{0: 0.0, 1: 1.0, 2: 2.0, 3: 3.0, 4: 4.0}\n"
     ]
    }
   ],
   "source": [
    "idx_encode = preprocessing.LabelEncoder()\n",
    "idx_encode.fit(train_label)\n",
    "\n",
    "y_train = idx_encode.transform(train_label) # 주어진 고유한 정수로 변환\n",
    "y_val = idx_encode.transform(val_label) # 고유한 정수로 변환\n",
    "\n",
    "label_idx = dict(zip(list(idx_encode.classes_), idx_encode.transform(list(idx_encode.classes_))))\n",
    "idx_label = {value: key for key, value in label_idx.items()}\n",
    "print(label_idx)\n",
    "print(idx_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TFBertForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"'klue/roberta-large'\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.initializers import TruncatedNormal\n",
    "from transformers import TFAutoModel\n",
    "\n",
    "class Klue_RobertaClassifier(Model):\n",
    "    def __init__(self, model_name):\n",
    "        super(Klue_RobertaClassifier, self).__init__()\n",
    "        self.bert = TFAutoModel.from_pretrained(\"klue/roberta-large\", num_labels=5, from_pt=True)\n",
    "        self.dropout = tf.keras.layers.Dropout(rate=0.5)\n",
    "        self.classifier = Dense(3, kernel_initializer=TruncatedNormal(0.02),activation='softmax') #가중치 초기화, 활성화함수\n",
    "\n",
    "    def call(self, inputs, attention_mask=None, token_type_ids=None, training=False):\n",
    "        input_ids, attention_mask, token_type_ids=inputs\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "        pooled_output = outputs[1]\n",
    "        pooled_output = self.dropout(pooled_output, training=training)\n",
    "        prediction = self.classifier(pooled_output)\n",
    "\n",
    "        return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFRobertaModel: ['lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.bias', 'roberta.embeddings.position_ids', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing TFRobertaModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFRobertaModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights or buffers of the TF 2.0 model TFRobertaModel were not initialized from the PyTorch model and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = Klue_RobertaClassifier(\"klue/roberta-large\")  #dropout=0.5까지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamWeightDecay\n",
    "optimizer = AdamWeightDecay(1e-5, weight_decay_rate=1e-4)  #과적합 방지\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=[metric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/keras/backend.py:4906: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "456/456 [==============================] - 371s 729ms/step - loss: nan - accuracy: 0.1969 - val_loss: nan - val_accuracy: 0.1932\n",
      "Epoch 2/3\n",
      "456/456 [==============================] - 330s 723ms/step - loss: nan - accuracy: 0.1966 - val_loss: nan - val_accuracy: 0.1932\n",
      "Epoch 3/3\n",
      "456/456 [==============================] - 329s 723ms/step - loss: nan - accuracy: 0.1966 - val_loss: nan - val_accuracy: 0.1932\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7c6da13f3430>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor=\"val_accuracy\", \n",
    "    min_delta=0.001,\n",
    "    patience=2)\n",
    "\n",
    "model.fit(\n",
    "    X_train, y_train, epochs=3, batch_size=8, validation_data=(X_valid, y_val),\n",
    "    callbacks = [early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 3)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " ...\n",
      " [nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n"
     ]
    }
   ],
   "source": [
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = [np.argmax(val) for val in pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0, 0.0, 0.0]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = [list(label_idx.keys())[_] for _ in result]\n",
    "out[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>아가씨 담배 한 갑 주소 네 4 500원입니다  어 네 지갑 어디 갔지  에이 버스...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>우리 팀에서 다른 팀으로 갈 사람 없나  그럼  영지씨가 가는 건 어때  네  제가...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>너 오늘 그게 뭐야 네 제가 뭘 잘못했나요  제대로 좀 하지 네 똑바로 좀 하지 행...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>이거 들어봐 와 이 노래 진짜 좋다 그치 요즘 이것만 들어 진짜 너무 좋다 내가 요...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>아무튼 앞으로 네가 내 와이파이야  응 와이파이 온  켰어  반말  주인님이라고도 ...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>그러니까 빨리 말해  선생님 제발 살려주십시오  비밀번호 틀릴 때마다 손톱 하나씩 ...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>그 사람 누구냐  누구 말 하는 거야  다 알면서 모른 척하지마  둘 다 쏴버리기 ...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>야 저기야 닭꼬치 트럭 왔다 응 그러네  그치  너도 먹고 싶지  어  나도  그래...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>안경 안 쓰고 키 작고 통통해 보이는 분이었는데 다 안경 안 쓴 쌍꺼풀 없었던 것 ...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>야 너 2학년 김민석 맞지  네 맞는데요  혹시 누구신가요  내가 누군지 궁금하면 ...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>예전에 네가 나한테 했던 일 기억하지  너도 당해봐 예전 일이라면 내가 사과할게  ...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>아 취업하고 싶은데 누가 나 좀 안 데려가냐 아  그러니까 나도 취업하고 싶다 코로...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>김 비서 지금 우리 애 학교 가서 집에다가 좀 데려다주세요  사장님 그런 개인적인 ...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>과장님 오늘 부장님 기분이 안 좋으신 것 같네요  오늘 프레젠테이션 있었는데 사장님...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>야 김 대리 니 주말에 뭐 했냐  아 그냥 쉬었습니다  아 그냥 쉬어  말 좀 짧다...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>후기 내리세요  안 그러면 고소합니다  지금 협박하시는 건가요  협박 아니고 알려주...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>응응 점점 깊이깊이 이야기가 들어갈수록 응응 맞아 마음이 좀 그럴 수 있을 것 같은...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>야 박보영 오늘 학원 갔다 왔어  아니 오늘 못 갔어  왜 아니 친구랑 놀다가 시간...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>좋은 말 할 때 내 돈 갚아  내가 왜 그래야 하지  네가 여기 재개발된다며  너 ...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>저기요  혹시 번호 좀 알려주실 수 있나요  네  아 죄송해요  저 남자 친구 있어...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text  class\n",
       "0   아가씨 담배 한 갑 주소 네 4 500원입니다  어 네 지갑 어디 갔지  에이 버스...    0.0\n",
       "1   우리 팀에서 다른 팀으로 갈 사람 없나  그럼  영지씨가 가는 건 어때  네  제가...    0.0\n",
       "2   너 오늘 그게 뭐야 네 제가 뭘 잘못했나요  제대로 좀 하지 네 똑바로 좀 하지 행...    0.0\n",
       "3   이거 들어봐 와 이 노래 진짜 좋다 그치 요즘 이것만 들어 진짜 너무 좋다 내가 요...    0.0\n",
       "4   아무튼 앞으로 네가 내 와이파이야  응 와이파이 온  켰어  반말  주인님이라고도 ...    0.0\n",
       "5   그러니까 빨리 말해  선생님 제발 살려주십시오  비밀번호 틀릴 때마다 손톱 하나씩 ...    0.0\n",
       "6   그 사람 누구냐  누구 말 하는 거야  다 알면서 모른 척하지마  둘 다 쏴버리기 ...    0.0\n",
       "7   야 저기야 닭꼬치 트럭 왔다 응 그러네  그치  너도 먹고 싶지  어  나도  그래...    0.0\n",
       "8   안경 안 쓰고 키 작고 통통해 보이는 분이었는데 다 안경 안 쓴 쌍꺼풀 없었던 것 ...    0.0\n",
       "9   야 너 2학년 김민석 맞지  네 맞는데요  혹시 누구신가요  내가 누군지 궁금하면 ...    0.0\n",
       "10  예전에 네가 나한테 했던 일 기억하지  너도 당해봐 예전 일이라면 내가 사과할게  ...    0.0\n",
       "11  아 취업하고 싶은데 누가 나 좀 안 데려가냐 아  그러니까 나도 취업하고 싶다 코로...    0.0\n",
       "12  김 비서 지금 우리 애 학교 가서 집에다가 좀 데려다주세요  사장님 그런 개인적인 ...    0.0\n",
       "13  과장님 오늘 부장님 기분이 안 좋으신 것 같네요  오늘 프레젠테이션 있었는데 사장님...    0.0\n",
       "14  야 김 대리 니 주말에 뭐 했냐  아 그냥 쉬었습니다  아 그냥 쉬어  말 좀 짧다...    0.0\n",
       "15  후기 내리세요  안 그러면 고소합니다  지금 협박하시는 건가요  협박 아니고 알려주...    0.0\n",
       "16  응응 점점 깊이깊이 이야기가 들어갈수록 응응 맞아 마음이 좀 그럴 수 있을 것 같은...    0.0\n",
       "17  야 박보영 오늘 학원 갔다 왔어  아니 오늘 못 갔어  왜 아니 친구랑 놀다가 시간...    0.0\n",
       "18  좋은 말 할 때 내 돈 갚아  내가 왜 그래야 하지  네가 여기 재개발된다며  너 ...    0.0\n",
       "19  저기요  혹시 번호 좀 알려주실 수 있나요  네  아 죄송해요  저 남자 친구 있어...    0.0"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['class'] = out\n",
    "test[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0    500\n",
       "Name: class, dtype: int64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['class'].value_counts() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission['class']=out\n",
    "submission.to_csv(\"klue-roberta-large-model.csv\", index=False)  #0.835"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
